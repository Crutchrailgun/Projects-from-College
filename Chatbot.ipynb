{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crutchrailgun/Projects-from-College/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Packages and some Memory Management\n",
        "\n"
      ],
      "metadata": {
        "id": "vAaDiFx9qZNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Author: Josiah Fout\n",
        "#Project: ChatBot\n",
        "#Start Date: 11/27/25\n",
        "#Hours Spent: 13\n",
        "\n",
        "!pip install -q transformers torch bitsandbytes sentencepiece huggingface_hub\n",
        "\n",
        "\n",
        "#An attempt to minimize memory usage\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:64\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9H_YgtDlPAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to HuggingFace"
      ],
      "metadata": {
        "id": "7A-Yc4I5qc88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "#Token was originally placed in Colab Secrets (for quick testing), but set it back to ideal setting for security.\n",
        "login()\n"
      ],
      "metadata": {
        "id": "7A0lCeArjB9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring LLaMA3 chatbot parameters (creating some variables)"
      ],
      "metadata": {
        "id": "BWjlBO7Rqh_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "dtype = torch.float16 #Data Type Used\n"
      ],
      "metadata": {
        "id": "8MDY0SBhY5XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline (Defines Structure/Behavior)\n",
        "\n",
        " *The message below is because I used a Colab Secret for my login during development so I didn't need to type it everytime, didn't effect the behavior.*"
      ],
      "metadata": {
        "id": "sBiU0Kf4rb_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\", #Structure/Behavior\n",
        "    model=model_id,\n",
        "    dtype=dtype, #Data Type as mentioned earlier\n",
        "    #Balances the usages between the resources of T4 (Disk, CPU, etc),\n",
        "    #automatically\n",
        "    device_map=\"auto\",\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "iXhdcEK1radS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#User Prompting, Messages, and Output Structure:\n",
        "**These individual steps were combined for greater ease of testing during development.**\n",
        "\n",
        "\n",
        "\n",
        "*Given the way my program is structured, I just ran the \"user-prompting\" part multiple times and printed the various responses. Figured this would be better than going the extra mile to save through files.*"
      ],
      "metadata": {
        "id": "uXvN3OSaLO4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap #For formatting\n",
        "\n",
        "#This section tends to have the most memory usage issues, given the amount of processing\n",
        "#--that happens in this block.\n",
        "\n",
        "\n",
        "question = input(\"Ask a question: \")\n",
        "\n",
        "# Establishes the chatbot's behavior and question needed.\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an AI chatbot helps college students. You have a very tactful and personality. You don't mince or waste words.\"},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "#Takes in the behavior and question structure to begin processing through the model\n",
        "out = pipeline(\n",
        "    messages, #Structure\n",
        "    max_new_tokens=256, #Allowed amount of new words/subwords for the model\n",
        "    temperature=0.7, #\"Creativity\" or \"Randomness\" of said Model\n",
        "    top_p=0.95, #Determines the size of the pool of words for model\n",
        "    do_sample=True, #Randomness\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "#Output, formated with textwrap and other to show the generated_text\n",
        "\n",
        "#Set Up response\n",
        "response = out[0][\"generated_text\"][-1][\"content\"]\n",
        "\n",
        "#This is probably not necessary but more formatting through\n",
        "#Constructing a List not unlike we gave it originally\n",
        "chat_history = []\n",
        "chat_history.append({\"role\": \"User\", \"content\": question})\n",
        "chat_history.append({\"role\": \"Assistant\", \"content\": response})\n",
        "\n",
        "#Printing and textwrapping through length determined loop\n",
        "for message in chat_history:\n",
        "    print(message[\"role\"] + \": \" + textwrap.fill((message[\"content\"])))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-lOZPnbILUv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Other Responses"
      ],
      "metadata": {
        "id": "KOQy5rM0tS0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for message in chat_history:\n",
        "    print(message[\"role\"] + \": \" + textwrap.fill((message[\"content\"])))\n",
        "\n"
      ],
      "metadata": {
        "id": "CBe3amqpBH0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in chat_history:\n",
        "  print(message[\"role\"] + \": \" + textwrap.fill((message[\"content\"])))\n"
      ],
      "metadata": {
        "id": "Qne6brn9BJw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot Experience\n"
      ],
      "metadata": {
        "id": "rYkFoad0voJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project was very fun! The Chatbot that I was able to come up with was able to answer my questions very well--even giving itself a name. I asked mainly math questions with one particuarly interesting question for the last one to see what it would come up with. The assignment itself was easy, but I found my primary struggle was overthinking the process itself or memory issues. All and all, I find the homework insightful and useful."
      ],
      "metadata": {
        "id": "bv9U7Iv9lSps"
      }
    }
  ]
}